\chapter{Typed Assembly Language, Safety and Formalizations}
\label{chap:introduction}

\section{The security ecosystem}
You would be hard pressed to find more than a handful of system on the internet
that has never contained a vulnerability. Many computers are even vulnerable to
known remote exploits for months or years at a time. As a result, a large
percentage of the computers on the internet are infected by some form of
malware. To try to combat these and similar problems, a number of tools have
been developed.

\begin{itemize}
\item Anti-virus software and intrusion detection systems try to catch malicious
  events in the making, by looking at e.g. network streams, files on disk and
  the behaviors of the programs running on a machine.
\item It is possible to use code-signing certificates and public-key
  infrastructure to prove that a program was written by a verified third-party.
\item Sandboxes and virtual machines are two examples of systems that try to
  limit what a program is \emph{able} to do at run-time. They do this by
  limiting the actions available to programs, for instance by not exposing
  dangerous APIs. Other examples from this category are POSIX user permissions,
  memory protections and firewalls. We will collectively refer to these and
  similar approaches as ``run-time protections''.  \footnote{While anti-virus
    and code-signing certificates principally belong to the category of run-time
    protections, they are sufficiently different from most other such
    protections that they deserve their own point.  }
  \todo{Monitors?}
\item Static analysis, fuzzing and type systems try to detect vulnerabilities in
  programs before they are deployed. Compiler modifcations such as stack-cookies
  or printf-hardening try to generate code that is hard to exploit even in the
  event of an exploit. By using a modern languages one might avoid whole classes
  of bugs. We will collectively refer to these and similar approaches as
  ``compile-time protections''.
\end{itemize}

\subsection{Trusted Computing Base}
A common concept among all protections is the Trusted Computing Base
  (TCB). The TCB is typically defined as the collection of software, hardware,
firmware, etc. that needs to be correct for a given protection to work. We have
two avenues for improving a TCB: We can reduce our assumptions by having fewer
or simpler components. Alternatively we might depend on multiple solutions,
where more than one failure is required to compromise the system. We call both
improvement a \emph{reduction} of the TCB, as both approaches will reduce the
requirements for the system to be correct.

As an example, consider a sandbox-solution. For this solution to be secure, we
require the sandbox, operating system and CPU to all be implemented
correctly. If we instead ran the program on a hypervisor, we would only require
a correct hypervisor and CPU. If the hypervisor is simpler than the OS and
sandbox combined, then we have reduced our TCB.

An obvious extension would be to allow proofs as part of our TCB. If we for
instance proved a program bug-free, then our TCB might be the CPU, operating
system and the proof itself. We include the proof, as it might not be
correct. The proof could employ a carefully hidden logical fallacy somewhere, it
might not model the CPU correctly -- or perhaps it did not imply the security
properties we thought it did. We could try to reduce this TCB by encoding our
proof in a proof engine. In this case any logical fallacy would \emph{also} need
to exploit a bug in the proof engine -- and the proof engine might have been
examined more carefully than the proof itself.

\subsection{Weaknesses of protections}
The above mentioned classes of protections are all valuable in the current
security ecosystem, however none of them are perfect. Most have very large TCBs
or can only be applied to specific use-cases. Additionally some of them are not
designed to \emph{prevent} attacks altogether, only to increase the cost of
doing them.

Anti-virus software is inherently limited by Rice's Theorem.\footnote{TODO:
  Rice's Theorem more generally states that: ... For an introduction see ...}
This theorem implies that any attempt to classify programs as good or bad will
have either false positives (good programs classified as bad) or false negatives
(bad programs classified as good). In practice most real-world implementations
have both. In addition there have been multiple examples of anti-virus software
decreasing security instead of increasing it.\footnote{TODO: Insert reference to
  project-zero lulz.}

The core idea of code-signing is to include a certificate with any distributed
programs. This certificate will prove that the program have been written by a
real-world entity verified by a trusted third-party using a public key
infrastructure (PKI). The purpose is to effectively increase the price of doing
an attack; ideally the only attack available would be to create a fake
real-world identity (expensive!) and use it to buy a code-signing
certificate. However in practice there are multiple other avenues available: One
could steal a valid certificate, exploit another vulnerable (but signed) program
or try to bypass the validation check altogether.

Run-time protections have a long history in computer science, and could arguably
be said to have had the biggest practical impact so far. One reason for their
effectiveness, is that they often allow many parallel protections (a technique
sometimes referred to as ``Defense in Depth''). It is for instance uncommon to
see end-user machines without memory protections, file permissions and sandboxes
for the most exposed applications among its protections. Another reason is that
this system is not (directly) limited by Rice's Theorem.\footnote{Many potential
  implementations \emph{would} be limited by Rice's Theorem, but that is not an
  inherent property of run-time protections. For instance if one simply wanted
  to stop a system from ``firing the missiles'', then one could cut the wire to
  the missile system.} One ``perfect'' system would have the user approve any
potentially malicious actions before execution -- however such a system would
not be practical. This illustrates the biggest problem with run-time
protections; one has to balance security with other priorities such as
performance, usability and ease of development.

Compile-time protections have also had huge influence in improving the quality
of the code generated and distributed in the world, though it is harder to
quantify exactly how much. With run-time protections one can often turn off a
specific protection (at least in principle) and look at how the change
influences the system. In contrast compile-time protections often work by
permanently improving code quality or are so integrated into the
compilation-process that they cannot be turned off. Many of these protections
are superficially similar to anti-virus software, as they work by analyzing code
and detecting potential problems, however they have an advantage in that they
are able to interact directly with the programmer. He can ignore false
positives, or he can improve the detection algorithms if he finds false
negatives. A disadvantage is that, even though this approach \emph{is} effective
at stopping problems, there is little possibility for users to \emph{verify}
that this is the case.

\section{Typed Assembly Language}

This thesis discusses a technique known as Typed Assembly Language (TAL). It was
first introduced in the paper ``From System F to Typed Assembly Language'' by
Morrisett et al. Its core idea is to add types to binary programs. By
distributing these types along with untrusted programs, one can verify certain
properties about the programs, and we can use this to build a system similar to
a sandbox. Specifically this system would, like sandboxes, take untrusted
programs as input and run them in a secure manner.

While the paper introduced a specific instance of TAL for a specific system, the
core idea is much more general. In fact it is possible to generate many
variations of it using the following procedure:

\begin{itemize}
\item Start by choosing a subset of a real-world assembly language.

\item Define a small-step semantics for this subset. A small-step semantics is a
  model for how to execute programs in the chosen subset. Our model might for
  instance state that a machine in state $p_1$ can transition to a machine in
  state $p_2$ (by e.g. running a single instruction), in which case we would
  write $p_1 -> p_2$. Furthermore this model will designate some states as
  valid, finished programs. This allows us to place any program state into
  exactly one of the these categories:

  \begin{itemize}
  \item \emph{Progressing} states are those that can transition to other states.
  \item \emph{Halting} states are the valid, finished states.
  \item \emph{Stuck} states are all other states. These are the states that can
    neither continue or halt in a valid way, and as such they are considered to
    be bad/error states.
  \end{itemize}

\item Define a type system for this subset. This is a way to designate e.g. that
  a register holds an integer instead of a pointer. We also want a type-checker
  for this type system. This is a procedure that, when given a program state and
  its type, will output whether the type matches the state.

  The type-checker is allowed to be wrong sometimes, as long as it will never
  output \texttt{valid} by mistake. We allow this, as this sometimes allows
  greater efficiency. If we can implement a type-checker without such errors,
  then we say that the type system is \emph{decidable}.

\item Prove that this type-system is \emph{sound}. This means that if $p_1$ is
  well-typed and if $p_1 -> \dots -> p_2$, then $p_2$ is not stuck.
\end{itemize}

The small-step semantics does not need to model the real-world perfectly. In
fact it can simplify quite a bit, as long as it agrees with the real-world
on some important points:

\begin{itemize}
\item If our model states that $p$ can transition to one of $p_1, \dots, p_n$
  (with $n \geq 1$), then an equivalent real-world machine should ideally do the
  same. We would want real-world equivalents of $p$ should transition to
  real-world equivalents of one of $p_1, \dots, p_n$. If this is not possible
  (for instance if the machine is out of memory), then the program \emph{must}
  be aborted in a non-harmful way instead.
\item If our model states that $p$ is halting, then the real world machine
  must halt without error.
\item If our model states that $p$ is stuck, then the real-world machine is
  allowed to do anything.
\item Our model should be chosen such that the ``bad stuff'' is left out. What
  is meant by ``bad stuff'' might vary between use-cases, though it will likely
  include accessing unmapped memory or executing attacker-controlled code. For
  an attempt at clarifying this and how one could hope to achieve it see
  \Cref{chap:safesound}.
\end{itemize}

If we create a TAL using this procedure, we can create our sandbox primitive as
follows:

\begin{enumerate}
\item Receive a program $p$.
\item Run the type-checker on $p$.
\item If the type-checker outputs false, then abort.
\item Compile or otherwise transform $p$ from a program in our model to a
  program for the real-world machine. In our case, this includes erasing the
  types and running an assembler on the result.
\item Run the transformed program without direct performance loss.
\end{enumerate}

If our soundness-proof and type-checker are correct and our model is valid, then
this sandbox is safe, and its TCB is potentially very small: It only needs a
correct model, soundness proof and type-checker. In practice the model will to
some extent depend on a run-time, so this run-time will also be a part of the
TCB.

We could try to evaluate this sandbox construction in the context of the
previous sections. If we do so, we see that it shares properties with all four
of the mentioned protection categories:

\begin{itemize}
\item TAL needs compile-time support to be useful. The compiler must type-check
  the source program and use these types when compiling the code.

\item The compiler uses the types to output a ``certificate'' along with the
  code. This certificate proves the program to be non-malicious. A difference is
  that this proof depends on a \emph{Trusted Computing Base}, not a
  \emph{Trusted Third Party}.

\item Before trying to run a program, it must first be classified as good or
  bad. This classification is limited by Rice's Theorem just like anti-virus
  software. Where anti-virus software will typically try to trade-off the false
  negatives with the false positives, this classifier will instead reject
  \emph{all} malicious programs (and some non-malicious ones). Another
  difference is that this classifier gets an additional input in the form of the
  ``certificate''.

\item Ultimately the system would have an API very similar to that of a
  sandbox. It would accept untrusted programs and run (or reject) them in a
  secure fashion. It would internally use the classifier, but ultimately this
  does not matter to an end-user.
\end{itemize}

\section{The Goal of the Thesis}
\label{sec:goal}

The goal of this thesis is ultimately how to design a secure sandbox primitive
based on Typed Assembly Language with specific focus on how to minimize the
TCB. This minimization takes two forms: By formalizing the meta-theory and by
trying to simplify the components needed beyond this formalization. As a result,
most of the work revolves around proofs (particularly proof of soundness) and
how they can be mechanized.

This thesis is \emph{not} about:

\begin{itemize}
\item How to implement infrastructure for this system beyond the sandbox
  itself. In particular we will not discuss compiler implementations.
\item How to reduce the TCB beyond reducing code size and formalizing the
  meta-theory. We will for example not talk about bugs in the hardware or CPU
  specification.
\item Any form of detailed analysis of the performance characteristics of the
  system.  We will specifically not do any thorough analysis of the algorithmic
  complexity of any algorithms, though we might informally argue why we believe
  that an algorithm will run within a certain bound.
\end{itemize}

\section{Outline of the Thesis}

The rest of this thesis is organized as follows:

\textbf{\Cref{chap:lang}} formally defines a variant of STAL, which we will
refer to as \ATAL (Agda-based TAL). \ATAL is mostly equivalent to STAL, however
support for exceptions and existential types have been removed, and a few
details have been simplified. While introducing \ATAL, we also introduce a
simplified version of the same language, \ATALe. After introducing the
languages, we will define small-step semantics for both
languages. \textbf{\Cref{chap:erasure}} will prove that the two semantics are
essentially equivalent. We do this by introducing the notion of erasure
semantics and prove that the two semantics are related by
it. \textbf{\Cref{chap:types}} introduces the type system for \ATAL and proves
that it is sound. We will also prove that the type system is
decidable. \textbf{\Cref{chap:safesound}} will discuss the issue of safety and
how it relates to the proof of soundness. Specifically we will give a high-level
sketch of how one could implement a platform with a small TCB from the results
in the previous chapters. \textbf{\Cref{chap:hindsight}} lists a few lessons
lessons from doing a project of this size in Agda. This includes both
observations specific to implement a TAL-like language in Agda, but also a few
more general tricks for making Agda easier to work with. It is meant to be
readable independently of the other chapters. \textbf{\Cref{chap:future}}
discusses how the work could be extended and relates it to other work. In
particular the differences between STAL and \ATAL will be discussed. Finally,
\textbf{\Cref{chap:conclusion}} summaries my contributions and concludes.
