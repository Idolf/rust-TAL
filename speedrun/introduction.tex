\chapter{Typed Assembly Language, Safety and Formalizations}
\label{chap:introduction}

\section{The security ecosystem}

Most computer systems in the world have at some point or another contained a
security vulnerability. As a result of this many computers on the internet is
infected by malware of some form. To try to combat these problems, we have
developed a number of tools:

\begin{itemize}
\item Anti-virus software and similar solutions try to catch malicious events in
  the making, by looking at e.g. network streams, files on disk and the
  behaviors of the programs running on a machine.
\item Code-signing certificates uses public-key infrastructure to prove that a
  program was written by a verified third-party.
\item Sandboxes and virtual machines two examples of systems that try to limit
  what a program is \emph{able} to do at run-time. They do this by not exposing
  APIs to do any undesired operations. Other examples from this category are
  POSIX user permissions, memory protections and firewalls. We will collectively
  refer to these and similar approaches as ``run-time protections''.
  \footnote{
    While anti-virus and code-signing certificates principally belong to the
    category of run-time protections, they are sufficiently different from most
    other such protections that they deserve their own point.
  }
\item Static analysis, fuzzing and type systems try to detect vulnerabilities in
  programs before they are deployed. Compiler modifcations stack-cookies or
  printf-hardening try to generate code that is hard to exploit even in the
  event of an exploit. Using modern languages one might even avoid whole classes
  of bugs. We will collectively refer to these and similar approaches as
  ``compile-time protections''.
\end{itemize}

Note, that while all of these are valuable tools in the current security
industry, none of them are perfect -- many are specifically designed to increase
the cost of attacks as much as possible, not preventing them altogether.

Anti-virus software is inherently limited by Rice's Theorem. This theorem
implies, that any attempt to classify programs as good or bad will have either
false positives (good programs classified as bad) or false negatives (bad
programs classified as good). In practice most real-world implementations have
both. In addition, anti-virus software is often of low quality which actually
results in increasing the attack surface of a system, not decreasing
it.\todo{Insert citation}

The core idea of code-signing is to include a certificate with any distributed
programs. This certificate will prove that the program have been written by a
real-world entity verified by a trusted third-party using a public key
infrastructure (PKI). The purpose is to effectively increase the price of
running malicious programs on other people's machines; ideally the only attack
available would be to create a fake real-world identity (expensive!) and use it
to buy a code-signing certificate. However in practice there are multiple other
avenues available: One could steal a valid certificate, exploit another
vulnerable (but signed) program or try to bypass the validation check.

Run-time protections have a long history in computer science, and could arguably
be said to have had the biggest practical impact so far. One reason for their
effectiveness, is that it is often possible to deploy many unrelated protections
in parallel (a technique sometimes referred to as ``Defense in Depth''). It is
for instance uncommon to see end-user machines without at least memory
protections, file permissions and sandboxes for the most exposed
applications. Another reason is that this system is not (directly) limited by
Rice's Theorem.\footnote{Many implementations \emph{would} be limited by Rice's
  Theorem, but that is not an inherent property of run-time protections. For
  instance if one simply wanted to stop a system from ``firing the missiles'',
  then one could cut the wire to do so.} It is for instance theoretically
possible to design system where any potentially malicious actions must be
approved by a user before execution -- however such a system would not be
practical. In general one has to balance security with other issues such as
performance, usability and ease of development.

Compile-time protections have also had huge influence in improving the quality
of the code generated and distributed in the world, though it is harder to
quantify exactly how much. With run-time protections one can often turn off a
protection (at least in principle) and look at the influence of that change. In
contrast compile-time protections often work by permanently improving code
quality or are so integrated into the compilation-process that they cannot be
turned off. Many of these protections are superficially similar to anti-virus
software, as they work by analyzing code and detecting potential problems,
however they have a huge advantage in that they are able to interact directly
with the programmer. He can choose to ignore warnings if he believes them to be
false positives, or he can improve the detection algorithms if he finds them
insufficient. A disadvantage is that while this approach \emph{does} improve
security in practice, there is little possibility for users to \emph{verify}
that this is the case.

This thesis discusses a technique known Typed Assembly Language (TAL). So where
in this classification system would TAL and variants fit? Ultimately they should
probably be grouped with the run-time protections, however they have properties
from each of the categories:

\begin{itemize}
\item TAL needs compiler-time support to be useful. The compiler must to
  type-check the source program and use these types when compiling the code.
\item The compiler then outputs a ``certificate'' along with this code. This
  certificate proves the program to be non-malicious. The difference is that
  this proof depends on a \emph{Trusted Computing Base}, not a \emph{Trusted
    Third Party}.
\item Before trying to run a program, it must first be classified as good or
  bad. This classification is limited by Rice's Theorem in the same way as an
  anti-virus solution. While anti-virus software will typically try to trade-off
  the false negatives with the false positives, this classifier will reject
  \emph{all} malicious programs (and some non-malicious ones). Another
  difference is that this classifier gets an additional input in the form of the
  ``certificate''.
\item Ultimately the system would have an API very similar to that of a
  sandbox. It would accept untrusted programs and run (or reject) them in a
  secure fashion. It would internally use the classifier, but ultimately this
  does not matter to an end-user.
\end{itemize}

\section{Typed Assembly Language}

Typed Assembly Language was introduced in the paper ``From System F to Typed
Assembly Language'' by Morrisett et al. The procedure set forth by this
technique is as follows:

\begin{itemize}
\item Start by choosing a subset of a real-world assembly language.
\item Define a small-step semantics for this subset. A small-step semantics is a
  model for how our subset is executed. For instance if our model states that a
  machine in state $p_1$ can transition to a machine in state $p_2$, then we
  would write $p_1 -> p_2$. Furthermore this model will designate some states as
  \emph{halting} states, i.e. states that will halt immediately without
  error. If programs are neither transitioning nor halting, then we say that
  they are \emph{stuck} and this is considered an error.
\item Define a type system for this subset. This type system should have a
  type-checker, i.e. a program that \emph{should} output true for well-typed
  programs, but \emph{always} outputs false for programs with type errors.
\item Prove that this type-system is \emph{sound}. This means that is $p_1$ is
  well-typed and $p_1 -> \dots -> p_2$, then $p_2$ is not stuck.
\end{itemize}

It is okay for our small-step semantics to be a simplification of the real
machine, as long it bears some relation to the real-world machine. Specifically
we say that the model is \emph{safe} if:
\begin{itemize}
\item If our model states that $p$ can transition to one of $p_1, \dots, p_n$
  (with $n \geq 1$), then the real-world machine should ideally transition to
  one of these states too. If it cannot do this (for instance if it is out of
  memory), then it should instead abort the program in a non-harmful way.
\item If our model states that $p$ is halting, then the real world machine
  should halt without error too.
\item If our model states that $p$ is stuck, then the real-world machine is
  allowed to do anything.
\item Our model should be chosen such that the ``bad stuff'' is left out. How to
  achieve this is discussed in \Cref{chap:safesound}.
\end{itemize}

We can now implement a sandbox as follows:

\begin{enumerate}
\item Receive a program $p$.
\item Run the type-checker on $p$.
\item If the type-checker outputs true, then we can safely run $p$.
\end{enumerate}

If our soundness-proof and type-checker are correct and our model is valid, then
this is safe.

\section{The Goals of the Thesis}
\label{sec:goals}

The goal of this thesis is ultimately how to design a secure sandbox primitive
based on Typed Assembly Language with specific focus on how to minimize the
Trusted Computing Base. This minimization takes two forms: By formalizing the
meta-theory and by trying to simplify the components needed beyond this
formalization. As a result most of the work revolves around proofs (particularly
proof of soundness) how they can be mechanized.

This thesis is \emph{not} about:

\begin{itemize}
\item How to implement infrastructure for this system beyond the sandbox
  itself. In particular we will not discuss compiler implementations.
\item How to reduce the Trusted Computing Base beyond reducing code size and
  formalizing the metatheory. We will for example not talk about bugs in the
  hardware or CPU specification.
\item Any form of detailed analysis of the performance characteristics of the
  system.  We will specifically not do any thorough analysis of the algorithmic
  complexity of any algorithms, though we might informally argue why we believe
  that an algorithm will run within a certain bound.
\end{itemize}

\section{Outline of the Thesis}

The rest of this thesis is organized as follows

\textbf{\Cref{chap:lang}} formally defines a variant of STAL, which we will
refer to as \ATAL (Agda-based TAL). \ATAL is mostly equivalent to STAL, however
support for exceptions and existential types have been removed, and a few
details have been simplified without loss of generality.

Simultaneously with introducing \ATAL, we will introduce a simplified version of
the same language, \ATALe, along with a small-step semantics for both
languages. We prove that the two languages are equivalent up to type erasure.

\textbf{\Cref{chap:types}} introduces the type system for \ATAL and proves that
it is sound. It will also introduce a decision procedure to check if a program
is well-typed (i.e. a proven-correct type-checker).

\textbf{\Cref{chap:safesound}} will further discuss the issue of safety and how
it relates to the proof of soundness. Specifically we will give a high-level
overview of how one could implement a platform with a small trusted computing
base from these results.

\textbf{\Cref{chap:hindsight}} lists a few lessons lessons from working doing a
project of this size in Agda. This includes both observations specific to
implement a TAL-like language in Agda, but also a few more general tricks for
making Agda easier to work with.

\textbf{\Cref{chap:future}} discusses how the work could be extended and relates
it to other work.

Finally, \textbf{\Cref{chap:conclusion}} summaries my contributions and
concludes.
